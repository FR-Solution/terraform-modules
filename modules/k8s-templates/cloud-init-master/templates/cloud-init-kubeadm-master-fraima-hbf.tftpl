#cloud-config
version: v1

users:
  - name: ${ ssh_username }
    sudo: ALL=(ALL) NOPASSWD:ALL
    groups: users, admin
    shell: /bin/bash
    lock_passwd: true
    ssh_authorized_keys:
      - ${ ssh_key }

packages:
  - tree
  - socat 
  - conntrack
  - ethtool
  - nftables

disk_setup:
  /dev/disk/by-id/virtio-etcd-data:
    table_type: 'gpt'
    overwrite: true

fs_setup:
  - label: etcd_data
    filesystem: 'ext4'
    device: /dev/disk/by-id/virtio-etcd-data
    partition: auto
    overwrite: true

mounts:
  - [ "/dev/disk/by-id/virtio-etcd-data", "/var/lib/etcd"]

runcmd:
  - mkdir /tmp/hbf
  - wget -O /tmp/hbf/hbf.tar.gz "${component_versions.bin.roy.bin_url}"
  - tar -xzf /tmp/hbf/hbf.tar.gz -C /tmp/hbf/
  - cp /tmp/hbf/bin/to-nft /usr/bin/hbf
  - chmod +x /usr/bin/hbf
  - systemctl enable  hbf.service
  - systemctl start   hbf.service
  # - sleep 15
  - mkdir -p /usr/bin/
  - chmod -R 700 /var/lib/etcd
  - mkdir -p /var/lib/key-keeper/vault
  - touch /var/lib/key-keeper/bootstrap.token
  - wget -O /tmp/fraimactl "${component_versions.bin.fraimactl.bin_url}"
  - tar -xzf /tmp/fraimactl -C /usr/bin/
  - chmod +x /usr/bin/fraimactl
  - fraimactl --config=/etc/kubernetes/fraimactl/config.yaml
  # - yc config profile create my-robot-profile
  # - yc config set cloud-id *
  # - yc config set folder-id *
  - export IAM_TOKEN=`yc iam create-token`
  - yc lockbox payload get --retry=200 --name=$(cat /proc/sys/kernel/hostname)-all-role-id   --key=$(cat /proc/sys/kernel/hostname)    > /var/lib/key-keeper/vault/all-role-id
  - yc lockbox payload get --retry=200 --name=$(cat /proc/sys/kernel/hostname)-all-secret-id --key=$(cat /proc/sys/kernel/hostname)    > /var/lib/key-keeper/vault/all-secret-id
  - systemctl enable  key-keeper.service
  - systemctl start   key-keeper.service
  - systemctl enable  systemd-resolved.service
  - systemctl start   systemd-resolved.service
  - until ls /etc/kubernetes/pki/ca/kubernetes-ca.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/ca/front-proxy-ca.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/ca/etcd-ca.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/ca/oidc-ca.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/etcd/etcd-peer-key.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/etcd/etcd-server-key.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/etcd/etcd-peer.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/etcd/etcd-server.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/kube-scheduler/kube-scheduler-server.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/kube-scheduler/kube-scheduler-client-key.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/kube-scheduler/kube-scheduler-client.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/kube-scheduler/kube-scheduler-server-key.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/kube-apiserver/kube-apiserver-etcd-client-key.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/kube-apiserver/front-proxy-client-key.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/kube-apiserver/front-proxy-client.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/kube-apiserver/kube-apiserver-sa.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/kube-apiserver/kube-apiserver-etcd-client.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/kube-apiserver/kube-apiserver-kubelet-client.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/kube-apiserver/kube-apiserver-kubelet-client-key.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/kube-apiserver/kubeadm-client-key.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/kube-apiserver/kubeadm-client.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/kube-apiserver/kube-apiserver-key.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/kube-apiserver/kube-apiserver.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/kube-controller-manager/kube-controller-manager-server.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/kube-controller-manager/kube-controller-manager-server-key.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/kube-controller-manager/kube-controller-manager-client-key.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/kube-controller-manager/kube-controller-manager-client.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/kubelet/kubelet-client-key.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/kubelet/kubelet-client.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/kubelet/kubelet-server-key.pem; do sleep 1; done
  - until ls /etc/kubernetes/pki/certs/kubelet/kubelet-server.pem; do sleep 1; done
  - kubeadm init --config=/etc/kubernetes/kubeadm/config.yaml


write_files:

####### Настройка key-keeper ###########################
###--->

  - path: /etc/systemd/system/key-keeper.service
    owner: root:root
    permissions: '0644'
    content: |
      ${indent(6, key-keeper-service)}

  - path: ${base_local_path_certs}/vault-config
    owner: root:root
    permissions: '0644'
    content: |
      ${indent(6, key-keeper-config)}

####### KUBECONFIGS для подключения к кластеру
  - path: ${main_path.base_kubernetes_path}/kube-scheduler/kubeconfig
    owner: root:root
    permissions: '0644'
    content: |
      ${indent(6, kube-scheduler-kubeconfig)}

  - path: ${main_path.base_kubernetes_path}/kube-controller-manager/kubeconfig
    owner: root:root
    permissions: '0644'
    content: |
      ${indent(6, kube-controller-manager-kubeconfig)}

  - path: /etc/kubernetes/admin.conf
    owner: root:root
    permissions: '0644'
    content: |
      ${indent(6, kube-apiserver-admin-kubeconfig)}

  - path: ${main_path.base_kubernetes_path}/kubelet/kubeconfig
    owner: root:root
    permissions: '0644'
    content: |
      ${indent(6, kubelet-kubeconfig)}

####### Настройка kubelet ##############################
###--->
  - path: ${main_path.base_kubernetes_path}/kubelet/config.yaml
    owner: root:root
    permissions: '0644'
    content: |
      ${indent(6, kubelet-config)}
###--->
####### *** ############################################


####### Статик поды для создания контрол плейна ########
###--->
  - path: ${base_path.base_static_pod_path}/etcd.yaml
    owner: root:root
    permissions: '0644'
    content: |
      ${indent(6, static-pod-etcd)}

  - path: /etc/kubernetes/kubeadm/config.yaml
    owner: root:root
    permissions: '0644'
    content: |
      ${indent(6, static-pod-kubeadm-config)}

###--->
####### *** ############################################    

####### Настройка cni ##############################
###--->
  - path: /etc/cni/net.d/99-loopback.conf
    owner: root:root
    permissions: '0644'
    content: |
      {
          "cniVersion": "0.4.0",
          "name": "lo",
          "type": "loopback"
      }
###--->
####### *** ############################################

  - path: /root/.bashrc
    owner: root:root
    permissions: '0644'
    content: |
      ${indent(6, bashrc-k8s )}

  - path: /etc/crictl.yaml
    owner: root:root
    permissions: '0644'
    content: |
      runtime-endpoint: unix:///run/containerd/containerd.sock

  - path: ${main_path.base_kubernetes_path}/kube-apiserver/audit-policy.yaml
    owner: root:root
    permissions: '0644'
    content: |
      ---
      apiVersion: audit.k8s.io/v1
      kind: Policy
      rules:
      - level: Metadata
      - level: RequestResponse

  - path: ${main_path.base_kubernetes_path}/fraimactl/config.yaml
    owner: root:root
    permissions: '0644'
    content: |
      - apiVersion: fraima.io/v1alpha
        kind: Containerd
        spec:
          service:
            extraArgs:
              # This document provides the description of the CRI plugin configuration. 
              # The CRI plugin config is part of the containerd config
              # Default: /etc/containerd/config.toml
              config: /etc/kubernetes/containerd/config.toml
          configuration:
            extraArgs:
              version: 2
              plugins:
                io.containerd.grpc.v1.cri:
                  containerd:
                    runtimes:
                      runc:
                        # Runtime v2 introduces a first class shim API for runtime authors to integrate with containerd. 
                        # The shim API is minimal and scoped to the execution lifecycle of a container.
                        runtime_type: "io.containerd.runc.v2"
                        options:
                          # While containerd and Kubernetes use the legacy cgroupfs driver for managing cgroups by default, 
                          # it is recommended to use the systemd driver on systemd-based hosts for compliance of the "single-writer" rule of cgroups. 
                          # To configure containerd to use the systemd driver, set the following option:
                          SystemdCgroup: true
          downloading:
            - name: containerd
              src: "${component_versions.bin.containerd.bin_url}"
              %{~ if try(component_versions.bin.containerd.sha256_url, "") != "" ~}
              checkSum:
                src: "${component_versions.bin.containerd.sha256_url}"
                type: "sha256"
              %{~ endif ~}
              path: /usr/bin/
              owner: root:root
              permission: 0645
              unzip:
                status: true
                files: 
                  - bin/containerd
                  - bin/containerd-shim
                  - bin/containerd-shim-runc-v1
                  - bin/containerd-shim-runc-v2
                  - bin/containerd-stress
                  - bin/ctr

            - name: runc
              src: "${component_versions.bin.runc.bin_url}"
              %{~ if try(component_versions.bin.runc.sha256_url, "") != "" ~}
              checkSum:
                src: "${component_versions.bin.runc.sha256_url}"
                type: "sha256"
              %{~ endif ~}              
              path: /usr/bin/
              owner: root:root
              permission: 0645
          starting:
            - systemctl enable containerd
            - systemctl start containerd


      - apiVersion: fraima.io/v1alpha
        kind: Kubelet
        spec:
          service:
            extraArgs:
              # Number for the log level verbosity
              v: 2
              # The directory where the TLS certs are located. 
              # If --tls-cert-file and --tls-private-key-file are provided, this flag will be ignored.
              # Default: /var/lib/kubelet/pki
              cert-dir: /etc/kubernetes/pki/certs/kubelet
              # The Kubelet will load its initial configuration from this file. 
              # The path may be absolute or relative; relative paths start at the Kubelet's current working directory. 
              # Omit this flag to use the built-in default configuration values. 
              # Command-line flags override configuration from this file.
              config: /etc/kubernetes/kubelet/config.yaml
              # Path to a kubeconfig file, specifying how to connect to the API server. 
              # Providing --kubeconfig enables API server mode, omitting --kubeconfig enables standalone mode.
              kubeconfig: /etc/kubernetes/kubelet/kubeconfig
              # Path to a kubeconfig file that will be used to get client certificate for kubelet. 
              # If the file specified by --kubeconfig does not exist, 
              # the bootstrap kubeconfig is used to request a client certificate from the API server. 
              # On success, a kubeconfig file referencing the generated client certificate and key is written to the path specified by --kubeconfig. 
              # The client certificate and key file will be stored in the directory pointed by --cert-dir.
              bootstrap-kubeconfig: /etc/kubernetes/kubelet/bootstrap-kubeconfig
              # The endpoint of remote runtime service. Unix Domain Sockets are supported on Linux, 
              # while npipe and tcp endpoints are supported on windows. 
              # Examples: unix:///path/to/runtime.sock, npipe:////./pipe/runtime.
              container-runtime-endpoint: /run/containerd/containerd.sock
              container-runtime: remote
              # Specified image will not be pruned by the image garbage collector. 
              # When container-runtime is set to docker, all containers in each pod will use the network/IPC namespaces from this image. 
              # Other CRI implementations have their own configuration to set this image.
              # Default: registry.k8s.io/pause:3.6
              pod-infra-container-image: k8s.gcr.io/pause:3.6
              # The provider for cloud services. Set to empty string for running with no cloud provider.
              # If set, the cloud provider determines the name of the node (consult cloud provider documentation to determine 
              # if and how the hostname is used). 
              # (DEPRECATED: will be removed in 1.24 or later, in favor of removing cloud provider code from Kubelet.)
              cloud-provider: external
          # configuration: {}
          downloading:
            - name: kubelet
              src: "${component_versions.bin.kubelet.bin_url}"
              %{~ if try(component_versions.bin.kubelet.sha256_url, "") != "" ~}
              checkSum:
                src: "${component_versions.bin.kubelet.sha256_url}"
                type: "sha256"
              %{~ endif ~}
              path: /usr/bin/
              owner: root:root
              permission: 0645
          starting:
            - systemctl enable kubelet
            - systemctl start kubelet

      - apiVersion: fraima.io/v1alpha
        kind: Sysctl
        spec:
          configuration:
            extraArgs:
              net.ipv4.ip_forward: 1
          starting:
            - sudo sysctl --system

      - apiVersion: fraima.io/v1alpha
        kind: Modprob
        spec:
          configuration:
            extraArgs:
            - br_netfilter
            - overlay
          starting:
            - sudo modprobe overlay
            - sudo modprobe br_netfilter
            - sudo sysctl --system

      - apiVersion: fraima.io/v1alpha
        kind: Kubeadm
        spec:
          downloading:
            - name: kubeadm
              src: "${component_versions.bin.kubeadm.bin_url}"
              %{~ if try(component_versions.bin.kubeadm.sha256_url, "") != "" ~}
              checkSum:
                src: "${component_versions.bin.kubeadm.sha256_url}"
                type: "sha256"
              %{~ endif ~}
              path: /usr/bin/
              owner: root:root
              permission: 0645

      - apiVersion: fraima.io/v1alpha
        kind: Kubectl
        spec:
          downloading:
            - name: kubectl
              src: "${component_versions.bin.kubectl.bin_url}"
              %{~ if try(component_versions.bin.kubectl.sha256_url, "") != "" ~}
              checkSum:
                src: "${component_versions.bin.kubectl.sha256_url}"
                type: "sha256"
              %{~ endif ~}
              path: /usr/bin/
              owner: root:root
              permission: 0645


      - apiVersion: fraima.io/v1alpha
        kind: Crictl
        spec:
          downloading:
            - name: crictl
              src: "${component_versions.bin.crictl.bin_url}"
              %{~ if try(component_versions.bin.crictl.sha256_url, "") != "" ~}
              checkSum:
                src: "${component_versions.bin.crictl.sha256_url}"
                type: "sha256"
              %{~ endif ~}
              path: /usr/bin/
              owner: root:root
              permission: 0645
              unzip:
                status: true
                files: 
                  - "crictl"

      - apiVersion: fraima.io/v1alpha
        kind: Etcdctl
        spec:
          downloading:
            - name: etcdctl
              src: "${component_versions.bin.etcdctl.bin_url}"
              %{~ if try(component_versions.bin.etcdctl.sha256_url, "") != "" ~}
              checkSum:
                src: "${component_versions.bin.etcdctl.sha256_url}"
                type: "sha256"
              %{~ endif ~}
              path: /usr/bin/
              owner: root:root
              permission: 0645
              unzip:
                status: true
                files: 
                  - "etcd-v3.5.5-linux-amd64/etcdctl"

      - apiVersion: fraima.io/v1alpha
        kind: KeyKeeper
        spec:
          downloading:
            - name: key-keeper
              src: "${component_versions.bin.key_keeper.bin_url}"
              %{~ if try(component_versions.bin.key_keeper.sha256_url, "") != "" ~}
              checkSum:
                src: "${component_versions.bin.key_keeper.sha256_url}"
                type: "sha256"
              %{~ endif ~}
              path: /usr/bin/
              owner: root:root
              permission: 0645
              unzip:
                status: true
                files: 
                  - "key-keeper"

      - apiVersion: fraima.io/v1alpha
        kind: YandexCloud
        spec:
          downloading:
            - name: yc
              src: "${component_versions.bin.yc.bin_url}"
              %{~ if try(component_versions.bin.yc.sha256_url, "") != "" ~}
              checkSum:
                src: "${component_versions.bin.yc.sha256_url}"
                type: "sha256"
              %{~ endif ~}
              path: /usr/bin/
              owner: root:root
              permission: 0645


  - path: /etc/systemd/system/hbf.service
    owner: root:root
    permissions: '0644'
    content: |
      [Unit]
      Description=hbf

      Wants=network-online.target
      After=network-online.target

      [Service]
      ExecStart=/usr/bin/hbf -config /etc/charlotte/config.yaml

      Restart=always
      StartLimitInterval=0
      RestartSec=10

      [Install]
      WantedBy=multi-user.target
      
  - path: /etc/charlotte/config.yaml
    owner: root:root
    permissions: '0644'
    content: |
      ---
      graceful-shutdown: 10s
      logger:
        level: INFO

      extapi:
        svc:
          def-daial-duration: 10s
          sgroups:
            dial-duration: 3s
            address: tcp://193.32.219.99:9000
            check-sync-status: 5s
